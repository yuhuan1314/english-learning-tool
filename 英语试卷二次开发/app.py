# è‹±è¯­è¯•å·äºŒæ¬¡å¼€å‘å·¥å…·
# è¿è¡Œå‘½ä»¤: streamlit run app.py

import streamlit as st
import pandas as pd
import io
import re
from docx import Document
from docx.shared import Pt, Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH

# ============ é¡µé¢é…ç½® ============
st.set_page_config(
    page_title="è‹±è¯­è¯•å·äºŒæ¬¡å¼€å‘å·¥å…·",
    page_icon="ğŸ“",
    layout="wide"
)

# ============ æ ·å¼ ============
st.markdown("""
<style>
    .main {
        background-color: #f5f5f5;
    }
    .stButton>button {
        background-color: #4CAF50;
        color: white;
        border-radius: 5px;
    }
    .title {
        text-align: center;
        color: #2c5282;
        font-size: 32px;
        font-weight: bold;
    }
    .section-header {
        background-color: #4299e1;
        color: white;
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
    }
    .exercise-box {
        background-color: white;
        padding: 15px;
        border-radius: 10px;
        border-left: 4px solid #4299e1;
        margin: 10px 0;
    }
</style>
""", unsafe_allow_html=True)

# ============ åŠŸèƒ½å‡½æ•° ============

def extract_text_from_pdf(pdf_file):
    """ä»PDFæå–æ–‡æœ¬"""
    try:
        import PyPDF2
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        st.error(f"PDFè§£æé”™è¯¯: {e}")
        return None

def identify_question_types(text):
    """è¯†åˆ«é¢˜å‹"""
    types = []
    text_lower = text.lower()
    
    if "é˜…è¯»ç†è§£" in text or "reading comprehension" in text_lower:
        types.append("é˜…è¯»ç†è§£")
    if "ä¸ƒé€‰äº”" in text or "ä¸ƒé€‰äº”" in text:
        types.append("ä¸ƒé€‰äº”")
    if "å®Œå½¢å¡«ç©º" in text or "cloze" in text_lower:
        types.append("å®Œå½¢å¡«ç©º")
    if "è¯­æ³•å¡«ç©º" in text or "grammar" in text_lower:
        types.append("è¯­æ³•å¡«ç©º")
    if "ä¹¦é¢è¡¨è¾¾" in text or "writing" in text_lower:
        types.append("ä¹¦é¢è¡¨è¾¾")
    if "å¬åŠ›" in text or "listening" in text_lower:
        types.append("å¬åŠ›")
    
    return types if types else ["é˜…è¯»ç†è§£", "ä¸ƒé€‰äº”", "å®Œå½¢å¡«ç©º", "è¯­æ³•å¡«ç©º"]

def extract_reading_articles(text):
    """æå–é˜…è¯»ç†è§£æ–‡ç« """
    articles = []
    # ç®€å•çš„æ–‡ç« æå–é€»è¾‘
    lines = text.split('\n')
    current_article = []
    in_article = False
    
    for line in lines:
        if any(marker in line for marker in ['A\n', 'B\n', 'C\n', 'D\n']) and len(line) < 5:
            if current_article:
                articles.append('\n'.join(current_article))
                current_article = []
            in_article = True
        if in_article:
            current_article.append(line)
    
    if current_article:
        articles.append('\n'.join(current_article))
    
    return articles[:4]  # æœ€å¤š4ç¯‡

def generate_vocabulary_exercise(article_text, topic):
    """ç”Ÿæˆè¯æ±‡ç»ƒä¹ """
    # æå–å…³é”®è¯
    words = re.findall(r'\b[a-zA-Z]{5,}\b', article_text.lower())
    word_freq = {}
    for w in words:
        word_freq[w] = word_freq.get(w, 0) + 1
    
    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
    
    return {
        "è¯é¢˜è¯æ±‡": [w[0] for w in top_words],
        "åŠ¨è¯": ["analyze", "explore", "discuss", "examine", "investigate"],
        "åè¯": ["concept", "principle", "theory", "perspective", "framework"],
        "å½¢å®¹è¯": ["significant", "essential", "fundamental", "crucial", "vital"]
    }

def generate_word_formation(article_text):
    """ç”Ÿæˆè¯æ€§å˜å½¢ç»ƒä¹ """
    return {
        "åŠ¨â†’å": ["discussâ†’discussion", "explainâ†’explanation", "analyzeâ†’analysis"],
        "åâ†’å½¢": ["traditionâ†’traditional", "cultureâ†’cultural", "natureâ†’natural"],
        "å‰¯è¯": ["completeâ†’completely", "trueâ†’truly", "happyâ†’happily"]
    }

def generate_phrase_translation():
    """ç”ŸæˆçŸ­è¯­ç¿»è¯‘"""
    return {
        "ä¸­æ–‡": ["ä¸ä»…...è€Œä¸”...", "äº‹å®ä¸Š", "é‡æ–°å®¡è§†", "çº¯ç²¹çš„ä¹è¶£", "å……å®çš„ç”Ÿæ´»"],
        "è‹±æ–‡": ["not only...but also...", "in fact", "revisit the question of...", "for pure pleasure, not purpose", "a life well lived"]
    }

def generate_cloze_exercise(article_text):
    """ç”Ÿæˆè¯­å¢ƒå¡«è¯"""
    words = ["however", "therefore", "although", "furthermore", "moreover", 
             "consequently", "nevertheless", "accordingly", "meanwhile", "otherwise"]
    selected = words[:8]
    
    exercise = {}
    for i, word in enumerate(selected, 1):
        exercise[f"{i}"] = {
            "word": word,
            "clue": f"é€‰æ‹©é€‚å½“çš„å•è¯å¡«ç©º{i}"
        }
    return exercise

def generate_discourse_structure(article_text):
    """ç”Ÿæˆè¯­ç¯‡ç»“æ„"""
    return {
        "å®è§‚éª¨æ¶": [
            {"æ®µè½": "Para 1", "åŠŸèƒ½": "å¼•è¨€", "å†…å®¹": "å¼•å‡ºä¸»é¢˜"},
            {"æ®µè½": "Para 2", "åŠŸèƒ½": "è®ºç‚¹1", "å†…å®¹": "ä¸»è¦è®ºç‚¹"},
            {"æ®µè½": "Para 3", "åŠŸèƒ½": "è®ºç‚¹2", "å†…å®¹": "æ”¯æŒè®ºæ®"},
            {"æ®µè½": "Para 4", "åŠŸèƒ½": "ç»“è®º", "å†…å®¹": "æ€»ç»“å‡å"}
        ],
        "è¡”æ¥è¯": [
            {"åŸå¥": "...is important.", "æŒ–ç©º": "________, ... is important.", "ç­”æ¡ˆ": "Therefore", "é€»è¾‘": "å› æœ"},
            {"åŸå¥": "However, ...", "æŒ–ç©º": "________, ...", "ç­”æ¡ˆ": "However", "é€»è¾‘": "è½¬æŠ˜"},
        ]
    }

def generate_micro_writing(topic, requirements):
    """ç”Ÿæˆå¾®å†™ä½œ"""
    return {
        "æƒ…å¢ƒ": f"å…³äº{topic}çš„å†™ä½œ",
        "è¯æ•°": "50è¯å·¦å³",
        "è¦æ±‚": requirements,
        "å¼€å¤´æç¤º": f"Write about {topic}...",
        "å‚è€ƒèŒƒæ–‡": f"This essay discusses {topic}. {topic} is an important concept that affects our daily life. In conclusion..."
    }

def generate_grammar_exercise(text):
    """ç”Ÿæˆè¯­æ³•ç»ƒä¹ """
    return {
        "éè°“è¯­åŠ¨è¯": ["ç°åœ¨åˆ†è¯ä½œçŠ¶è¯­", "è¿‡å»åˆ†è¯ä½œå®šè¯­", "ä¸å®šå¼ä½œç›®çš„çŠ¶è¯­"],
        "å®šè¯­ä»å¥": ["which/thatå¼•å¯¼", "who/whomå¼•å¯¼", "ä»‹è¯+which/whom"],
        "çŠ¶è¯­ä»å¥": ["æ—¶é—´çŠ¶è¯­", "åŸå› çŠ¶è¯­", "è®©æ­¥çŠ¶è¯­", "ç»“æœçŠ¶è¯­"],
        "åè¯æ€§ä»å¥": ["ä¸»è¯­ä»å¥", "å®¾è¯­ä»å¥", "è¡¨è¯­ä»å¥", "åŒä½è¯­ä»å¥"]
    }

def create_word_document(exercises_data, article_title):
    """åˆ›å»ºWordæ–‡æ¡£"""
    doc = Document()
    
    # æ ‡é¢˜
    title = doc.add_heading(f'{article_title} - äºŒæ¬¡å¼€å‘ç»ƒä¹ ', 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # è¯æ±‡ç»ƒä¹ 
    doc.add_heading('ä¸€ã€è¯é¢˜è¯æ±‡', level=1)
    if "vocabulary" in exercises_data:
        for category, words in exercises_data["vocabulary"].items():
            doc.add_paragraph(f"{category}: {', '.join(words)}")
    
    # è¯æ€§å˜å½¢
    doc.add_heading('äºŒã€è¯æ€§å˜å½¢', level=1)
    if "word_formation" in exercises_data:
        for transform_type, examples in exercises_data["word_formation"].items():
            doc.add_paragraph(f"{transform_type}:")
            for ex in examples:
                doc.add_paragraph(f"  â€¢ {ex}", style='List Bullet')
    
    # çŸ­è¯­ç¿»è¯‘
    doc.add_heading('ä¸‰ã€çŸ­è¯­ç¿»è¯‘', level=1)
    if "phrase_translation" in exercises_data:
        table = doc.add_table(rows=len(exercises_data["phrase_translation"]["ä¸­æ–‡"])+1, cols=2)
        table.style = 'Table Grid'
        table.rows[0].cells[0].text = 'ä¸­æ–‡'
        table.rows[0].cells[1].text = 'è‹±æ–‡'
        for i, (cn, en) in enumerate(zip(exercises_data["phrase_translation"]["ä¸­æ–‡"], 
                                          exercises_data["phrase_translation"]["è‹±æ–‡"]), 1):
            table.rows[i].cells[0].text = cn
            table.rows[i].cells[1].text = en
    
    # è¯­ç¯‡ç»“æ„
    doc.add_heading('å››ã€è¯­ç¯‡ç»“æ„', level=1)
    doc.add_paragraph('å®è§‚éª¨æ¶ï¼š')
    if "discourse" in exercises_data and "å®è§‚éª¨æ¶" in exercises_data["discourse"]:
        for item in exercises_data["discourse"]["å®è§‚éª¨æ¶"]:
            doc.add_paragraph(f"  {item['æ®µè½']}: {item['åŠŸèƒ½']} - {item['å†…å®¹']}")
    
    doc.add_paragraph('è¡”æ¥è¯æŒ–ç©ºï¼š')
    if "discourse" in exercises_data and "è¡”æ¥è¯" in exercises_data["discourse"]:
        for item in exercises_data["discourse"]["è¡”æ¥è¯"]:
            doc.add_paragraph(f"  {item['æŒ–ç©º']} â†’ ç­”æ¡ˆ: {item['ç­”æ¡ˆ']} ({item['é€»è¾‘']})")
    
    # å¾®å†™ä½œ
    doc.add_heading('äº”ã€å¾®å†™ä½œ', level=1)
    if "micro_writing" in exercises_data:
        mw = exercises_data["micro_writing"]
        doc.add_paragraph(f"æƒ…å¢ƒ: {mw['æƒ…å¢ƒ']}")
        doc.add_paragraph(f"è¯æ•°: {mw['è¯æ•°']}")
        doc.add_paragraph(f"è¦æ±‚: {mw['è¦æ±‚']}")
        doc.add_paragraph(f"å¼€å¤´æç¤º: {mw['å¼€å¤´æç¤º']}")
        doc.add_paragraph(f"å‚è€ƒèŒƒæ–‡: {mw['å‚è€ƒèŒƒæ–‡']}")
    
    # ä¿å­˜
    return doc

# ============ ä¸»ç•Œé¢ ============

st.markdown('<p class="title">ğŸ“ è‹±è¯­è¯•å·äºŒæ¬¡å¼€å‘å·¥å…·</p>', unsafe_allow_html=True)

st.markdown("---")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    
    st.subheader("é¢˜å‹é€‰æ‹©")
    question_types = st.multiselect(
        "é€‰æ‹©è¦å¤„ç†çš„é¢˜å‹",
        ["é˜…è¯»ç†è§£", "ä¸ƒé€‰äº”", "å®Œå½¢å¡«ç©º", "è¯­æ³•å¡«ç©º", "ä¹¦é¢è¡¨è¾¾"],
        default=["é˜…è¯»ç†è§£"]
    )
    
    st.sub_header("ç»ƒä¹ ç±»å‹")
    include_vocab = st.checkbox("è¯é¢˜è¯æ±‡", value=True)
    include_word_form = st.checkbox("è¯æ€§å˜å½¢", value=True)
    include_phrase = st.checkbox("çŸ­è¯­ç¿»è¯‘", value=True)
    include_cloze = st.checkbox("è¯­å¢ƒå¡«è¯", value=True)
    include_discourse = st.checkbox("è¯­ç¯‡ç»“æ„", value=True)
    include_writing = st.checkbox("å¾®å†™ä½œ", value=True)
    
    st.markdown("---")
    st.info("ğŸ’¡ æç¤ºï¼šä¸Šä¼ PDFè¯•å·åï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«é¢˜å‹å¹¶ç”Ÿæˆç›¸åº”çš„äºŒæ¬¡å¼€å‘ç»ƒä¹ ã€‚")

# ä¸»åŒºåŸŸ
st.header("ğŸ“¤ ä¸Šä¼ è¯•å·")

uploaded_file = st.file_uploader("é€‰æ‹©PDFæ–‡ä»¶", type=['pdf'])

if uploaded_file:
    st.success("âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼")
    
    with st.spinner("ğŸ” æ­£åœ¨è§£æè¯•å·..."):
        text = extract_text_from_pdf(uploaded_file)
    
    if text:
        st.info(f"âœ… è¯†åˆ«åˆ°è¯•å·å†…å®¹ï¼Œé•¿åº¦: {len(text)} å­—ç¬¦")
        
        # è¯†åˆ«é¢˜å‹
        identified_types = identify_question_types(text)
        st.write(f"ğŸ“‹ è¯†åˆ«åˆ°çš„é¢˜å‹: {', '.join(identified_types)}")
        
        # æ˜¾ç¤ºé€‰é¡¹
        selected_articles = st.selectbox(
            "é€‰æ‹©æ–‡ç« è¿›è¡ŒäºŒæ¬¡å¼€å‘",
            ["Aç¯‡", "Bç¯‡", "Cç¯‡", "Dç¯‡", "ä¸ƒé€‰äº”", "å®Œå½¢å¡«ç©º", "è¯­æ³•å¡«ç©º"]
        )
        
        # ç”Ÿæˆç»ƒä¹ 
        if st.button("ğŸš€ ç”ŸæˆäºŒæ¬¡å¼€å‘ç»ƒä¹ ", type="primary"):
            with st.spinner("âœï¸ æ­£åœ¨ç”Ÿæˆç»ƒä¹ ..."):
                # æ„å»ºç»ƒä¹ æ•°æ®
                exercises = {}
                
                if include_vocab:
                    exercises["vocabulary"] = generate_vocabulary_exercise(text, selected_articles)
                
                if include_word_form:
                    exercises["word_formation"] = generate_word_formation(text)
                
                if include_phrase:
                    exercises["phrase_translation"] = generate_phrase_translation()
                
                if include_cloze:
                    exercises["cloze"] = generate_cloze_exercise(text)
                
                if include_discourse:
                    exercises["discourse"] = generate_discourse_structure(text)
                
                if include_writing:
                    exercises["micro_writing"] = generate_micro_writing(
                        selected_articles, 
                        "è¿ç”¨æœ¬è¯¾è¯é¢˜è¯æ±‡ï¼Œä½¿ç”¨Withå¤åˆç»“æ„"
                    )
                
                # æ˜¾ç¤ºç»ƒä¹ 
                st.markdown("---")
                st.header("ğŸ“š ç”Ÿæˆçš„ç»ƒä¹ ")
                
                # è¯æ±‡
                if include_vocab and "vocabulary" in exercises:
                    st.markdown('<div class="section-header">ä¸€ã€è¯é¢˜è¯æ±‡</div>', unsafe_allow_html=True)
                    for category, words in exercises["vocabulary"].items():
                        with st.expander(f"{category}"):
                            st.write(", ".join(words))
                
                # è¯æ€§å˜å½¢
                if include_word_form and "word_formation" in exercises:
                    st.markdown('<div class="section-header">äºŒã€è¯æ€§å˜å½¢</div>', unsafe_allow_html=True)
                    for transform_type, examples in exercises["word_formation"].items():
                        st.markdown(f"**{transform_type}**")
                        for ex in examples:
                            st.write(f"  â€¢ {ex}")
                
                # çŸ­è¯­ç¿»è¯‘
                if include_phrase and "phrase_translation" in exercises:
                    st.markdown('<div class="section-header">ä¸‰ã€çŸ­è¯­ç¿»è¯‘</div>', unsafe_allow_html=True)
                    pt = exercises["phrase_translation"]
                    df = pd.DataFrame({"ä¸­æ–‡": pt["ä¸­æ–‡"], "è‹±æ–‡": pt["è‹±æ–‡"]})
                    st.table(df)
                
                # è¯­ç¯‡ç»“æ„
                if include_discourse and "discourse" in exercises:
                    st.markdown('<div class="section-header">å››ã€è¯­ç¯‡ç»“æ„</div>', unsafe_allow_html=True)
                    
                    st.subheader("å®è§‚éª¨æ¶")
                    ds = exercises["discourse"]
                    if "å®è§‚éª¨æ¶" in ds:
                        for item in ds["å®è§‚éª¨æ¶"]:
                            st.write(f"**{item['æ®µè½']}**: {item['åŠŸèƒ½']} - {item['å†…å®¹']}")
                    
                    st.subheader("è¡”æ¥è¯æŒ–ç©º")
                    if "è¡”æ¥è¯" in ds:
                        for item in ds["è¡”æ¥è¯"]:
                            st.text_input(
                                item['æŒ–ç©º'], 
                                value=item['ç­”æ¡ˆ'],
                                help=f"é€»è¾‘: {item['é€»è¾‘']}"
                            )
                
                # å¾®å†™ä½œ
                if include_writing and "micro_writing" in exercises:
                    st.markdown('<div class="section-header">äº”ã€å¾®å†™ä½œ</div>', unsafe_allow_html=True)
                    mw = exercises["micro_writing"]
                    st.write(f"**æƒ…å¢ƒ**: {mw['æƒ…å¢ƒ']}")
                    st.write(f"**è¯æ•°**: {mw['è¯æ•°']}")
                    st.write(f"**è¦æ±‚**: {mw['è¦æ±‚']}")
                    st.write(f"**å¼€å¤´æç¤º**: {mw['å¼€å¤´æç¤º']}")
                    with st.expander("å‚è€ƒèŒƒæ–‡"):
                        st.write(mw['å‚è€ƒèŒƒæ–‡'])
                
                # å¯¼å‡º
                st.markdown("---")
                st.header("ğŸ’¾ å¯¼å‡º")
                
                # åˆ›å»ºWordæ–‡æ¡£
                doc = create_word_document(exercises, selected_articles)
                
                # ä¿å­˜åˆ°å†…å­˜
                doc_buffer = io.BytesIO()
                doc.save(doc_buffer)
                doc_buffer.seek(0)
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½Wordæ–‡æ¡£",
                    data=doc_buffer,
                    file_name=f"{selected_articles}_äºŒæ¬¡å¼€å‘ç»ƒä¹ .docx",
                    type="primary"
                )

else:
    st.info("ğŸ‘† è¯·ä¸Šä¼ PDFæ ¼å¼çš„è‹±è¯­è¯•å·æ–‡ä»¶")
    
    # ç¤ºä¾‹
    st.markdown("---")
    st.header("ğŸ“‹ ç¤ºä¾‹")
    st.write("""
    ### ç”Ÿæˆçš„ç»ƒä¹ åŒ…å«ï¼š
    
    1. **è¯é¢˜è¯æ±‡** - åŠ¨ä½œå¼•æ“ã€è¯­ä¹‰æ”¯æŸ±ã€ä¿®é¥°è¯
    2. **è¯æ€§å˜å½¢** - åŠ¨â†’åã€åâ†’å½¢ã€å‰¯è¯
    3. **çŸ­è¯­ç¿»è¯‘** - æ•´å—é€»è¾‘çŸ­è¯­
    4. **è¯­å¢ƒå¡«è¯** - åŸºäºæ–‡ç« çš„æŒ–ç©ºç»ƒä¹ 
    5. **è¯­ç¯‡ç»“æ„** - å®è§‚éª¨æ¶ã€è¡”æ¥è¯æŒ–ç©º
    6. **å¾®å†™ä½œ** - 50è¯æƒ…å¢ƒå†™ä½œ
    """)

# é¡µè„š
st.markdown("---")
st.markdown("<div style='text-align: center; color: #666;'>è‹±è¯­è¯•å·äºŒæ¬¡å¼€å‘å·¥å…· | åŸºäºå…‹æ‹‰ç”³è¾“å…¥å‡è¯´ä¸å¸ƒé²å§†è®¤çŸ¥æ¨¡å‹</div>", unsafe_allow_html=True)
